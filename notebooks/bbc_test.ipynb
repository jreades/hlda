{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical LDA Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how we can load the BBC Insight Dataset (http://mlg.ucd.ie/datasets/bbc.html), preprocess them via NLTK and run hierarchical LDA inference on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "basedir = '..'\n",
    "sys.path.append(os.path.join(basedir))\n",
    "\n",
    "import pylab as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "from hlda.sampler import HierarchicalLDA\n",
    "from ipywidgets import widgets\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "import string\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load test corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess text using NLTK. Below we load all tech articles from the BBC Insight corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopset = stopwords.words('english') + list(string.punctuation) + ['will', 'also', 'said']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    corpus = []\n",
    "    all_docs = []\n",
    "    vocab = set()\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    for filename in glob.glob(os.path.join(basedir,'bbc','tech','*.txt')):\n",
    "        with open(filename) as f:\n",
    "            try:\n",
    "\n",
    "                doc = f.read().splitlines() \n",
    "                doc = filter(None, doc) # remove empty string\n",
    "                doc = '. '.join(doc)\n",
    "\n",
    "                to_remove = string.punctuation\n",
    "                table = str.maketrans(\"\", \"\", to_remove)\n",
    "                doc = doc.translate(table) # strip punctuations\n",
    "\n",
    "                to_remove = \"0123456789\"\n",
    "                table = str.maketrans(\"\", \"\", to_remove)\n",
    "                doc = doc.translate(table) # strip numbers\n",
    "\n",
    "                doc = doc.encode(\"utf8\").decode('ascii', 'ignore') # ignore fancy unicode chars\n",
    "                all_docs.append(doc)        \n",
    "\n",
    "                tokens = word_tokenize(str(doc))\n",
    "                filtered = []\n",
    "                for w in tokens:\n",
    "                    w = stemmer.stem(w.lower()) # use Porter's stemmer\n",
    "                    if len(w) < 3:              # remove short tokens\n",
    "                        continue\n",
    "                    if w in stopset:            # remove stop words\n",
    "                        continue\n",
    "                    filtered.append(w)\n",
    "\n",
    "                vocab.update(filtered)\n",
    "                corpus.append(filtered)      \n",
    "\n",
    "            except UnicodeDecodeError:\n",
    "                print('Failed to load', filename)\n",
    "                \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inverted index for the words to position in the sorted vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = sorted(list(vocab))\n",
    "vocab_index = {}\n",
    "for i, w in enumerate(vocab):\n",
    "    vocab_index[w] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(all_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of vocab. Also print the first 100 words in the sorted vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(vocab))\n",
    "print(\", \".join([vocab[v] for v in np.random.randint(0, len(vocab), 100, dtype='int')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some pretty word cloud using the Python Word Cloud package: https://github.com/amueller/word_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white').generate(' '.join(all_docs))\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run hLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(vocab), len(corpus), len(corpus[0]), len(corpus[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert words in the corpus into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "for doc in corpus:\n",
    "    new_doc = []\n",
    "    for word in doc:\n",
    "        word_idx = vocab_index[word]\n",
    "        new_doc.append(word_idx)\n",
    "    new_corpus.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(vocab), len(new_corpus))\n",
    "print(corpus[0][0:10])\n",
    "print(new_corpus[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hierarchical LDA object and run the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_samples = 500       # no of iterations for the sampler\n",
    "alpha = 10.0          # smoothing over level distributions\n",
    "gamma = 1.0           # CRP smoothing parameter; number of imaginary customers at next, as yet unused table\n",
    "eta = 0.1             # smoothing over topic-word distributions\n",
    "num_levels = 3        # the number of levels in the tree\n",
    "display_topics = 50   # the number of iterations between printing a brief summary of the topics so far\n",
    "n_words = 5           # the number of most probable words to print for each topic after model estimation\n",
    "with_weights = False  # whether to print the words with the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hlda = HierarchicalLDA(new_corpus, vocab, alpha=alpha, gamma=gamma, eta=eta, num_levels=num_levels)\n",
    "hlda.estimate(n_samples, display_topics=display_topics, n_words=n_words, with_weights=with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colour_map = {\n",
    "    0: 'blue',\n",
    "    1: 'red',\n",
    "    2: 'green'\n",
    "}\n",
    "\n",
    "def show_doc(d=0):\n",
    "    \n",
    "    node = hlda.document_leaves[d]\n",
    "    path = []\n",
    "    while node is not None:\n",
    "        path.append(node)\n",
    "        node = node.parent\n",
    "    path.reverse()   \n",
    "    \n",
    "    n_words = 10\n",
    "    with_weights = False    \n",
    "    for n in range(len(path)):\n",
    "        node = path[n]\n",
    "        colour = colour_map[n] \n",
    "        msg = 'Level %d Topic %d: ' % (node.level, node.node_id)\n",
    "        msg += node.get_top_words(n_words, with_weights)\n",
    "        output = '<h%d><span style=\"color:%s\">%s</span></h3>' % (n+1, colour, msg)\n",
    "        display(HTML(output))\n",
    "        \n",
    "    display(HTML('<hr/><h5>Processed Document</h5>'))\n",
    "\n",
    "    doc = corpus[d]\n",
    "    output = ''\n",
    "    for n in range(len(doc)):\n",
    "        w = doc[n]\n",
    "        l = hlda.levels[d][n]\n",
    "        colour = colour_map[l]\n",
    "        output += '<span style=\"color:%s\">%s</span> ' % (colour, w)\n",
    "    display(HTML(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this notebook locally, you'd be able to flip through the documents in the corpus and see the topic assignments of individual words of the document...\n",
    "\n",
    "#### No longer seems to work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "widgets.interact(show_doc, d=(0, len(corpus)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dump the hlda object for further use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/18474791/decreasing-the-size-of-cpickle-objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import gzip\n",
    "\n",
    "def save_zipped_pickle(obj, filename, protocol=-1):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        cPickle.dump(obj, f, protocol)\n",
    "        \n",
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = cPickle.load(f)\n",
    "        return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_zipped_pickle(hlda, 'bbc_hlda.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Topic Modelling",
   "language": "python",
   "name": "tpm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "03d4a94715d647d1ad07cd0817285e4f": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
